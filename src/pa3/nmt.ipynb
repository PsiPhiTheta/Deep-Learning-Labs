{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "s9IS9B9-yUU5",
        "9DaTdRNuUra7",
        "4BIpGwANoQOg",
        "pbvpn4MaV0I1",
        "bRWfRdmVVjUl",
        "bXNsLNkOn38w",
        "_BAfi_8yWB3y",
        "tWe0RO5FWajD",
        "kiUwiOITHTW4",
        "hmQmyJDSRFKR",
        "7cP7nl5NRJbu",
        "X8FaZZUWRpY9",
        "qbfZCByITOI6"
      ]
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "TjPTaRB4mpCd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignmentby selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "metadata": {
        "id": "s9IS9B9-yUU5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup PyTorch\n",
        "All files are stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "colab_type": "code",
        "outputId": "c9be808f-57ab-4596-80aa-45a6fb0a292f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow==4.0.0\n",
        "%mkdir -p /content/csc421/a3/\n",
        "%cd /content/csc421/a3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python2.7/dist-packages (1.0.1.post2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python2.7/dist-packages (0.2.2.post3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python2.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python2.7/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Collecting Pillow==4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/99/0e3522a9764fe371bf9f7729404b1ef7d9c4fc49cbe5f1761c6e07812345/Pillow-4.0.0-cp27-cp27mu-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.6MB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python2.7/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mfastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.0.1.post2 which is incompatible.\u001b[0m\n",
            "\u001b[31mimgaug 0.2.8 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mtorchvision 0.2.2.post3 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 4.1.1\n",
            "    Uninstalling Pillow-4.1.1:\n",
            "      Successfully uninstalled Pillow-4.1.1\n",
            "Successfully installed Pillow-4.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/csc421/a3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9DaTdRNuUra7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Helper code"
      ]
    },
    {
      "metadata": {
        "id": "4BIpGwANoQOg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Utility functions"
      ]
    },
    {
      "metadata": {
        "id": "D-UJHBYZkh7f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "        \n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
        "        pkl.dump(idx_dict, f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pbvpn4MaV0I1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data loader"
      ]
    },
    {
      "metadata": {
        "id": "XVT4TNTOV3Eg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\n",
        "    \"\"\"\n",
        "    lines = open(filename).read().strip().lower().split('\\n')\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
        "    \"\"\"\n",
        "    return all(c.isalpha() or c == '-' for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
        "    \"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n",
        "    \"\"\"\n",
        "\n",
        "    source_lines, target_lines = read_pairs('data/pig_latin_data.txt')\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index['SOS'] = start_token\n",
        "    char_to_index['EOS'] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = { 'char_to_index': char_to_index,\n",
        "                 'index_to_char': index_to_char,\n",
        "                 'start_token': start_token,\n",
        "                 'end_token': end_token }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s,t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s,t))\n",
        "\n",
        "    return d\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bRWfRdmVVjUl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "metadata": {
        "id": "wa5-onJhoSeM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
        "    \"\"\"\n",
        "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\n",
        "    \"\"\"\n",
        "\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "\n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "      \n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "    \n",
        "    if isinstance(attention_weights, tuple):\n",
        "      ## transformer's attention mweights\n",
        "      attention_weights, self_attention_weights = attention_weights\n",
        "    \n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "    \n",
        "    for i in range(len(all_attention_weights)):\n",
        "      attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "      fig = plt.figure()\n",
        "      ax = fig.add_subplot(111)\n",
        "      cax = ax.matshow(attention_weights_matrix, cmap='bone')\n",
        "      fig.colorbar(cax)\n",
        "\n",
        "      # Set up axes\n",
        "      ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n",
        "      ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n",
        "\n",
        "      # Show label at every tick\n",
        "      ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      # Add title\n",
        "      plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n",
        "      plt.tight_layout()\n",
        "      plt.grid('off')\n",
        "      plt.show()\n",
        "      #plt.savefig(save)\n",
        "\n",
        "      #plt.close(fig)\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
        "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n",
        "            \n",
        "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, encoder_hidden)\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "              # Zero gradients\n",
        "              optimizer.zero_grad()\n",
        "              # Compute gradients\n",
        "              loss.backward()\n",
        "              # Update the parameters of the encoder and decoder\n",
        "              optimizer.step()\n",
        "              \n",
        "    mean_loss = np.mean(losses)\n",
        "    return mean_loss\n",
        "\n",
        "  \n",
        "\n",
        "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "        \n",
        "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
        "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, train_loss, val_loss, gen_string))\n",
        "\n",
        "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        save_loss_plot(train_losses, val_losses, opts)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Data Stats'.center(80))\n",
        "    print('-' * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
        "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
        "    print('Vocab size: {}'.format(vocab_size))\n",
        "    print('=' * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data()\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    encoder = GRUEncoder(vocab_size=vocab_size, \n",
        "                         hidden_size=opts.hidden_size, \n",
        "                         opts=opts)\n",
        "\n",
        "    if opts.decoder_type == 'rnn':\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, \n",
        "                             hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == 'rnn_attention':\n",
        "        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
        "                                      hidden_size=opts.hidden_size, \n",
        "                                      attention_type=opts.attention_type)\n",
        "    elif opts.decoder_type == 'transformer':\n",
        "        decoder = TransformerDecoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    #### setup checkpoint path\n",
        "    model_name = 'h{}-bs{}-{}'.format(opts.hidden_size, \n",
        "                                      opts.batch_size, \n",
        "                                      opts.decoder_type)\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
        "\n",
        "    try:\n",
        "        training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return encoder, decoder\n",
        "      \n",
        "    return encoder, decoder\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bXNsLNkOn38w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Your code for NMT models"
      ]
    },
    {
      "metadata": {
        "id": "_BAfi_8yWB3y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## GRU cell"
      ]
    },
    {
      "metadata": {
        "id": "9ztmyA5Ro67o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyGRUCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        ## Input linear layers\n",
        "        self.Wiz = nn.Linear(input_size, hidden_size)\n",
        "        self.Wir = nn.Linear(input_size, hidden_size)\n",
        "        self.Wih = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        ## Hidden linear layers\n",
        "        self.Whz = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Whr = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Whh = nn.Linear(hidden_size, hidden_size)\n",
        "        # ------------        \n",
        "\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"Forward pass of the GRU computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        z = torch.sigmoid(self.Wiz(x) + self.Whz(h_prev)) # could also use F.Sigmoid (depricated)\n",
        "        r = torch.sigmoid(self.Wir(x) + self.Whr(h_prev))\n",
        "        g = torch.tanh(self.Wih(x) + r*self.Whh(h_prev))\n",
        "        h_new = (1-z)*g + z*h_prev\n",
        "        return h_new\n",
        "        # ------------\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-JBVFLEZWNC1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### GRU encoder / decoder"
      ]
    },
    {
      "metadata": {
        "id": "xaDt7XDmWRzC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GRUEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(GRUEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = nn.GRUCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
        "            hidden = self.gru(x, hidden)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)\n",
        "\n",
        "\n",
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None\n",
        "        \"\"\"        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tWe0RO5FWajD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ]
    },
    {
      "metadata": {
        "id": "9GUK5A7CWhV8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "                                    nn.Linear(hidden_size*2, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(hidden_size, 1)\n",
        "                                 )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        batch_size = queries.size(0) # could also do keys.shape\n",
        "        expanded_queries = queries.unsqueeze(1).expand_as(keys) # keys.size() gives ERROR! queries 2D --> 3D dimention needs to be same shape as keys dimension, both 3D\n",
        "        concat_inputs = torch.cat((expanded_queries, keys),2) # try with 1\n",
        "        unnormalized_attention = self.attention_network(concat_inputs)\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.squeeze(2).unsqueeze(1), values) # or torch.sum(attention_weights*values, 1)\n",
        "        return context, attention_weights\n",
        "        # ------------\n",
        "      \n",
        "\n",
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        batch_size = queries.size(0)\n",
        "        q = self.Q(queries.unsqueeze(1)) if len(queries.size()) == 2 else self.Q(queries) # W_q * Q_t\n",
        "        k = self.K(keys) # W_k K-i\n",
        "        v = self.V(values) # V_i\n",
        "        \n",
        "        unnormalized_attention = k.bmm(q.permute(0,2,1)) * self.scaling_factor \n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.permute(0,2,1), v)\n",
        "        return context, attention_weights\n",
        "        # ------------\n",
        "      \n",
        "      \n",
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
        "            \n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        batch_size = queries.size(0)\n",
        "        q = self.Q(queries.unsqueeze(1)) if len(queries.size()) == 2 else self.Q(queries)\n",
        "        k = self.K(keys) \n",
        "        v = self.V(values) \n",
        "        unnormalized_attention = k.bmm(q.permute(0, 2, 1)) * self.scaling_factor \n",
        "        mask = torch.tril(torch.ones_like(unnormalized_attention), diagonal=-1) * self.neg_inf\n",
        "        attention_weights = self.softmax(unnormalized_attention + mask)\n",
        "        context =  attention_weights.permute(0, 2, 1).bmm(v)\n",
        "        return context, attention_weights\n",
        "        # ------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pemjZo2XWtRt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Attention decoder"
      ]
    },
    {
      "metadata": {
        "id": "PfjF0Z-PWwPv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
        "        if attention_type == 'additive':\n",
        "          self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == 'scaled_dot':\n",
        "          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            # ------------\n",
        "            embed_current = embed[:,i,:]\n",
        "            context, attention_weights = self.attention(h_prev, annotations, annotations)\n",
        "            embed_and_context = torch.cat((context.squeeze(1), embed_current),1) \n",
        "            h_prev = self.rnn(embed_and_context, h_prev)\n",
        "            # ------------\n",
        "            \n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N8JpcwTRW5cw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transformer decoder"
      ]
    },
    {
      "metadata": {
        "id": "V5vJPku1W7sz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([CausalScaledDotAttention( #currently in causal mode, for non-causal: self.self_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        for i in range(self.num_layers):\n",
        "          # ------------\n",
        "          new_contexts, self_attention_weights = self.self_attentions[i](contexts, contexts, contexts)\n",
        "          residual_contexts = new_contexts + contexts\n",
        "          new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts, annotations, annotations)\n",
        "          residual_contexts = residual_contexts + new_contexts\n",
        "          new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "          contexts = residual_contexts + new_contexts\n",
        "          # ------------\n",
        "\n",
        "          encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "          self_attention_weights_list.append(self_attention_weights)\n",
        "          \n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "        \n",
        "        return output, (encoder_attention_weights, self_attention_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XuNFd6LNo0-o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training\n"
      ]
    },
    {
      "metadata": {
        "id": "kiUwiOITHTW4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download dataset"
      ]
    },
    {
      "metadata": {
        "id": "xwcFjsEpHRbI",
        "colab_type": "code",
        "outputId": "9d616ba0-7bfe-4ed0-cbba-00b5de2c940a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='pig_latin_data.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_data.txt', \n",
        "                         untar=False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/pig_latin_data.txt\n",
            "('Downloading data from', 'http://www.cs.toronto.edu/~jba/pig_latin_data.txt')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hmQmyJDSRFKR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## RNN decoder"
      ]
    },
    {
      "metadata": {
        "id": "0LKaRF1jwhH7",
        "colab_type": "code",
        "outputId": "b1ca1c72-a5f8-4a3d-d448-1ec667393bb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2193
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_encoder, rnn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: rnn                                    \n",
            "                               lr_decay: 0.99                                   \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.375 | Val loss: 1.993 | Gen: esteray ay ongeray ay ongeray\n",
            "Epoch:   1 | Train loss: 1.917 | Val loss: 1.832 | Gen: estedray antedray ontedray antedray oontedray\n",
            "Epoch:   2 | Train loss: 1.760 | Val loss: 1.725 | Gen: eray aredway ontedray ingsay ontedray\n",
            "Epoch:   3 | Train loss: 1.654 | Val loss: 1.660 | Gen: edray aredway ontersay ingray onterstay\n",
            "Epoch:   4 | Train loss: 1.578 | Val loss: 1.614 | Gen: edway aredway onterstay ingray onterstay\n",
            "Epoch:   5 | Train loss: 1.516 | Val loss: 1.564 | Gen: edway aredway onstertay indway othedway\n",
            "Epoch:   6 | Train loss: 1.460 | Val loss: 1.529 | Gen: edway ainday ontinghtray indway oodgay\n",
            "Epoch:   7 | Train loss: 1.417 | Val loss: 1.508 | Gen: edway aidway ontestionday inway oodgay\n",
            "Epoch:   8 | Train loss: 1.380 | Val loss: 1.496 | Gen: edway aistay ontestionday inway othestay\n",
            "Epoch:   9 | Train loss: 1.362 | Val loss: 1.474 | Gen: edway aissay ontellay inway othedtay\n",
            "Epoch:  10 | Train loss: 1.331 | Val loss: 1.440 | Gen: edway aidway ontertingway inway ortrestray\n",
            "Epoch:  11 | Train loss: 1.304 | Val loss: 1.428 | Gen: edway aidway ontellay inway ortrethtay\n",
            "Epoch:  12 | Train loss: 1.271 | Val loss: 1.438 | Gen: eway aishay ontertionssay inway otrortay\n",
            "Epoch:  13 | Train loss: 1.255 | Val loss: 1.405 | Gen: estray aidway ontertingway inway otromway\n",
            "Epoch:  14 | Train loss: 1.236 | Val loss: 1.381 | Gen: eday aidway ontingstay inatedway ortray-ortingway\n",
            "Epoch:  15 | Train loss: 1.225 | Val loss: 1.400 | Gen: echay aidedway ontingssay inway onthompray\n",
            "Epoch:  16 | Train loss: 1.213 | Val loss: 1.359 | Gen: yeay aidedway ontingstay-owssay inateday otromway\n",
            "Epoch:  17 | Train loss: 1.183 | Val loss: 1.366 | Gen: echedway aidway ontingstay inatedway otromway\n",
            "Epoch:  18 | Train loss: 1.161 | Val loss: 1.336 | Gen: yeay aidway ontingstay-ay-owspra iway otromway\n",
            "Epoch:  19 | Train loss: 1.150 | Val loss: 1.357 | Gen: yeyday aidway ontionstay inay ortimentway\n",
            "Epoch:  20 | Train loss: 1.131 | Val loss: 1.310 | Gen: yeyday aidway ontionstay iway ortresttray\n",
            "Epoch:  21 | Train loss: 1.133 | Val loss: 1.317 | Gen: yeay aidedway ontingsonay inay ortrestray\n",
            "Epoch:  22 | Train loss: 1.115 | Val loss: 1.315 | Gen: echedray airedway ontingertssay inay ortremngentway\n",
            "Epoch:  23 | Train loss: 1.156 | Val loss: 1.409 | Gen: eceday airedway otay-ay-oway-inway inway orfertstay\n",
            "Epoch:  24 | Train loss: 1.146 | Val loss: 1.301 | Gen: ethay aidedway ontingsay-ay-ortay iway orfertstay\n",
            "Epoch:  25 | Train loss: 1.084 | Val loss: 1.276 | Gen: echedgay aidedway ontingertssay iway orfertstay\n",
            "Epoch:  26 | Train loss: 1.072 | Val loss: 1.259 | Gen: echedway aidway ontingerstay iway orfertssay\n",
            "Epoch:  27 | Train loss: 1.053 | Val loss: 1.261 | Gen: echednay aidedway ontistionstay inay orfermentway\n",
            "Epoch:  28 | Train loss: 1.072 | Val loss: 1.277 | Gen: echednay aidway ontistionssay inay orfingray\n",
            "Epoch:  29 | Train loss: 1.062 | Val loss: 1.274 | Gen: echtray aidway ontistionssay inay orfingway\n",
            "Epoch:  30 | Train loss: 1.040 | Val loss: 1.237 | Gen: echednay aidway ontingerstay iway orfingedway\n",
            "Epoch:  31 | Train loss: 1.037 | Val loss: 1.279 | Gen: echtray airway ontingingway inationday orfingway\n",
            "Epoch:  32 | Train loss: 1.034 | Val loss: 1.256 | Gen: echay airway ontistertay inay orfermay\n",
            "Epoch:  33 | Train loss: 1.011 | Val loss: 1.229 | Gen: ethay airway ontistertay inay orfingedway\n",
            "Epoch:  34 | Train loss: 1.004 | Val loss: 1.217 | Gen: echay airway ontingerstay iway orfingray\n",
            "Epoch:  35 | Train loss: 0.999 | Val loss: 1.212 | Gen: echednay airway ontistionstay inway orfingeray\n",
            "Epoch:  36 | Train loss: 1.001 | Val loss: 1.239 | Gen: echednay airway ontisingsay iway orrkingway\n",
            "Epoch:  37 | Train loss: 0.999 | Val loss: 1.253 | Gen: echedgray airway ontistionstay inationway orrfingednay\n",
            "Epoch:  38 | Train loss: 0.973 | Val loss: 1.220 | Gen: echednay airway ontistionstay iway orkingray\n",
            "Epoch:  39 | Train loss: 0.960 | Val loss: 1.204 | Gen: echednay airway ontistionstay inationssay orrkingway\n",
            "Epoch:  40 | Train loss: 0.956 | Val loss: 1.232 | Gen: echay aidray ontistionstay iway orkingray\n",
            "Epoch:  41 | Train loss: 0.956 | Val loss: 1.231 | Gen: ethay airway onsitingway inatway orfingssay\n",
            "Epoch:  42 | Train loss: 0.967 | Val loss: 1.197 | Gen: eceway airway ontisterstray iway orfermay\n",
            "Epoch:  43 | Train loss: 0.942 | Val loss: 1.219 | Gen: echedgray airway ontisterstray inay orfredmay\n",
            "Epoch:  44 | Train loss: 0.936 | Val loss: 1.186 | Gen: echedway airway onsitingstay iway orrfermay\n",
            "Epoch:  45 | Train loss: 0.928 | Val loss: 1.197 | Gen: ethay airway ontistermay inssay orfridedway\n",
            "Epoch:  46 | Train loss: 0.919 | Val loss: 1.182 | Gen: echedgay airway ontistionsday iway orrfingeday\n",
            "Epoch:  47 | Train loss: 0.914 | Val loss: 1.176 | Gen: echedgay airway onsitingstay isay orfridedway\n",
            "Epoch:  48 | Train loss: 0.907 | Val loss: 1.186 | Gen: echedway airway onsitingsay isay orrferstray\n",
            "Epoch:  49 | Train loss: 0.943 | Val loss: 1.214 | Gen: ethay airway onsitingsay inway orfingray\n",
            "Epoch:  50 | Train loss: 0.935 | Val loss: 1.173 | Gen: echedway airway onsititybay iway orrkingway\n",
            "Epoch:  51 | Train loss: 0.920 | Val loss: 1.172 | Gen: echednay airway onsitingsbray isay orrifmay\n",
            "Epoch:  52 | Train loss: 0.909 | Val loss: 1.170 | Gen: echfay airway onitistedway isay orfingeday\n",
            "Epoch:  53 | Train loss: 0.890 | Val loss: 1.164 | Gen: echedway airway onsitionsday isay orkingsay\n",
            "Epoch:  54 | Train loss: 0.881 | Val loss: 1.159 | Gen: echedway airway onsitionsday iway orrfinedway\n",
            "Epoch:  55 | Train loss: 0.868 | Val loss: 1.144 | Gen: ethay airway onsistionsday isay orrifmay\n",
            "Epoch:  56 | Train loss: 0.867 | Val loss: 1.153 | Gen: ethay airway onsitionsday iway orkingsay\n",
            "Epoch:  57 | Train loss: 0.875 | Val loss: 1.159 | Gen: ethay airway onsitionsday isay orfingedway\n",
            "Epoch:  58 | Train loss: 0.869 | Val loss: 1.153 | Gen: ethay airway onsitionsday isay orfingedray\n",
            "Epoch:  59 | Train loss: 0.876 | Val loss: 1.137 | Gen: ethay airway onisitionmay iway orfingedway\n",
            "Epoch:  60 | Train loss: 0.863 | Val loss: 1.138 | Gen: ethay airway onistingshay isay orfrinday\n",
            "Epoch:  61 | Train loss: 0.846 | Val loss: 1.142 | Gen: ethay airway onsitingshay isay orkingray\n",
            "Epoch:  62 | Train loss: 0.840 | Val loss: 1.139 | Gen: ethay airway onsistionstray isay orrymentway\n",
            "Epoch:  63 | Train loss: 0.830 | Val loss: 1.135 | Gen: ethay airway onisitionmay isay orrifmay\n",
            "Epoch:  64 | Train loss: 0.823 | Val loss: 1.143 | Gen: ethay airway onsitingshay isay orrifway\n",
            "Epoch:  65 | Train loss: 0.827 | Val loss: 1.144 | Gen: ethay airway onisitionmay isay orrifmay\n",
            "Epoch:  66 | Train loss: 0.832 | Val loss: 1.134 | Gen: ethay airway onisitingway iway orrifmay\n",
            "Epoch:  67 | Train loss: 0.843 | Val loss: 1.119 | Gen: ethay airway onisitionmay isay orringmay\n",
            "Epoch:  68 | Train loss: 0.878 | Val loss: 1.138 | Gen: ethay airway onsistingshay iway orrkingray\n",
            "Epoch:  69 | Train loss: 0.821 | Val loss: 1.137 | Gen: ethay airway onsistingshay isay orrifway\n",
            "Epoch:  70 | Train loss: 0.822 | Val loss: 1.122 | Gen: ethay airway onisitredstray isay orrifysay\n",
            "Epoch:  71 | Train loss: 0.816 | Val loss: 1.124 | Gen: ethay airway onisitrednay isay orrifmay\n",
            "Epoch:  72 | Train loss: 0.805 | Val loss: 1.131 | Gen: ethay airway onsistingshay isay orrifway\n",
            "Epoch:  73 | Train loss: 0.794 | Val loss: 1.132 | Gen: ethay airway onsistingshay isay orrifmay\n",
            "Epoch:  74 | Train loss: 0.785 | Val loss: 1.123 | Gen: ethay airway onisitingway isay orrifmay\n",
            "Epoch:  75 | Train loss: 0.785 | Val loss: 1.142 | Gen: ethay airway onsitionsday isay orrkingray\n",
            "Epoch:  76 | Train loss: 0.813 | Val loss: 1.122 | Gen: ethay airway onsitionsday isay orkingray\n",
            "Epoch:  77 | Train loss: 0.811 | Val loss: 1.126 | Gen: ethay airway onisitingway isay orringmay\n",
            "Epoch:  78 | Train loss: 0.798 | Val loss: 1.134 | Gen: ethay airway onsistingshay iway orrifnay\n",
            "Epoch:  79 | Train loss: 0.780 | Val loss: 1.101 | Gen: ethay airway onsistingshay isay orringfay\n",
            "Epoch:  80 | Train loss: 0.772 | Val loss: 1.115 | Gen: ethay airway onsitionsday isay orringfay\n",
            "Epoch:  81 | Train loss: 0.768 | Val loss: 1.113 | Gen: ethay airway onisitingway isay orringmay\n",
            "Epoch:  82 | Train loss: 0.760 | Val loss: 1.117 | Gen: ethay airway onsitionsday isay orringfay\n",
            "Epoch:  83 | Train loss: 0.760 | Val loss: 1.111 | Gen: ethay airway onisitingway isay orringmay\n",
            "Epoch:  84 | Train loss: 0.761 | Val loss: 1.109 | Gen: ethay airway onisitredstay isay orringemay\n",
            "Epoch:  85 | Train loss: 0.760 | Val loss: 1.122 | Gen: ethay airway onisitredray isay orkingray\n",
            "Epoch:  86 | Train loss: 0.763 | Val loss: 1.112 | Gen: ethay airway onisitingway isay orringmay\n",
            "Epoch:  87 | Train loss: 0.797 | Val loss: 1.133 | Gen: ethay airway onsitionsday isay orryinfay\n",
            "Epoch:  88 | Train loss: 0.785 | Val loss: 1.108 | Gen: ethay airway onisitingway isay orringmay\n",
            "Epoch:  89 | Train loss: 0.760 | Val loss: 1.084 | Gen: ethay airway onsitionsday isay orkingray\n",
            "Epoch:  90 | Train loss: 0.755 | Val loss: 1.109 | Gen: ethay airway onisitrednay iway orringmay\n",
            "Epoch:  91 | Train loss: 0.747 | Val loss: 1.106 | Gen: ethay airway onisitrednay isay orkingray\n",
            "Epoch:  92 | Train loss: 0.743 | Val loss: 1.130 | Gen: ethay airway onsitionsday isay orkingray\n",
            "Epoch:  93 | Train loss: 0.752 | Val loss: 1.099 | Gen: ethay airway onsitionsday isay orkingray\n",
            "Epoch:  94 | Train loss: 0.733 | Val loss: 1.106 | Gen: ethay airway onisitrednay isay orringmay\n",
            "Epoch:  95 | Train loss: 0.735 | Val loss: 1.090 | Gen: ethay airway onsitionsday isay orkingray\n",
            "Epoch:  96 | Train loss: 0.729 | Val loss: 1.099 | Gen: ethay airway onisitedmay isay orkingray\n",
            "Epoch:  97 | Train loss: 0.725 | Val loss: 1.082 | Gen: ethay airway onsistingshay isay orkingsay\n",
            "Epoch:  98 | Train loss: 0.745 | Val loss: 1.118 | Gen: ethay airway onisitionday isay ornifsingray\n",
            "Epoch:  99 | Train loss: 0.786 | Val loss: 1.101 | Gen: ethay airway onisitedmay isay orkingray\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onisitedmay isay orkingray\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vmXAefS0qDoD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "p2kPGj5DFv7a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the british-born sixty nine year old is known as the godfather of the strain of artificial intelligence called neural networks or neural nets which involves setting up computer systems to mimic the human brain allowing them to learn it is some experts say going to radically transform our lives already is actually the way electricity did'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7cP7nl5NRJbu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## RNN attention decoder"
      ]
    },
    {
      "metadata": {
        "id": "nKlyfbuPDXDR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2329
        },
        "outputId": "eb2b5b78-5f09-4bed-ef8e-151ab1d34576"
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'additive',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_attn_encoder, rnn_attn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: rnn_attention                          \n",
            "                               lr_decay: 0.99                                   \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type GRUEncoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type RNNAttentionDecoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type MyGRUCell. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type AdditiveAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0 | Train loss: 2.351 | Val loss: 1.942 | Gen: eray ay inessay inssay inessay\n",
            "Epoch:   1 | Train loss: 1.850 | Val loss: 1.706 | Gen: eray ay ingesingay isisisisisisisisisis onstestestay\n",
            "Epoch:   2 | Train loss: 1.586 | Val loss: 1.423 | Gen: eray aitsay oningingay istay oringsay\n",
            "Epoch:   3 | Train loss: 1.347 | Val loss: 1.219 | Gen: eday ay-ioray ontiningay isthay orsinghay\n",
            "Epoch:   4 | Train loss: 1.114 | Val loss: 1.015 | Gen: etay ay-iray ondicinghay issay orsinghay\n",
            "Epoch:   5 | Train loss: 0.929 | Val loss: 0.871 | Gen: etay airway ondicingay issay orkinghay\n",
            "Epoch:   6 | Train loss: 0.831 | Val loss: 0.819 | Gen: etay airsay onditiongay issay orkinghay\n",
            "Epoch:   7 | Train loss: 0.757 | Val loss: 0.785 | Gen: ethay airway onditingay ishay orkinghay\n",
            "Epoch:   8 | Train loss: 0.703 | Val loss: 0.876 | Gen: etay airway onditiongay issay orkingsay\n",
            "Epoch:   9 | Train loss: 0.665 | Val loss: 0.638 | Gen: etay airway onditioningay ishay orkinghay\n",
            "Epoch:  10 | Train loss: 0.567 | Val loss: 0.666 | Gen: ethay airway onditingay ispay orkingway\n",
            "Epoch:  11 | Train loss: 0.498 | Val loss: 0.561 | Gen: ethay airway onditioningay issay orkinghay\n",
            "Epoch:  12 | Train loss: 0.520 | Val loss: 0.486 | Gen: ethay airway onditioningway issay orkingsay\n",
            "Epoch:  13 | Train loss: 0.396 | Val loss: 0.482 | Gen: etthay airay ondpitioningway ispray orkinglay\n",
            "Epoch:  14 | Train loss: 0.416 | Val loss: 0.550 | Gen: eththay iarsay onditiningcay ishay orkingsay\n",
            "Epoch:  15 | Train loss: 0.423 | Val loss: 0.435 | Gen: eththay airway onditioningway issay orkinghay\n",
            "Epoch:  16 | Train loss: 0.355 | Val loss: 0.502 | Gen: etay airay-ay onditioningway issay orkingway\n",
            "Epoch:  17 | Train loss: 0.450 | Val loss: 0.436 | Gen: etay airray onditioningway issay orkingway\n",
            "Epoch:  18 | Train loss: 0.325 | Val loss: 0.335 | Gen: etay iariray onditioningway isway orkingpay\n",
            "Epoch:  19 | Train loss: 0.277 | Val loss: 0.367 | Gen: etay airway onditioningway issay orkingpray\n",
            "Epoch:  20 | Train loss: 0.257 | Val loss: 0.346 | Gen: etay airway onditioningway isway orkingway\n",
            "Epoch:  21 | Train loss: 0.263 | Val loss: 0.438 | Gen: etay airway onditioningway issay orkingway\n",
            "Epoch:  22 | Train loss: 0.248 | Val loss: 0.364 | Gen: etay airway onditioningway isay orkingway\n",
            "Epoch:  23 | Train loss: 0.219 | Val loss: 0.385 | Gen: etay airway onditioningway isway orkingway\n",
            "Epoch:  24 | Train loss: 0.334 | Val loss: 0.501 | Gen: eththhay airway onditiongway isway orkingsay\n",
            "Epoch:  25 | Train loss: 0.305 | Val loss: 0.410 | Gen: etay airway onditiongnay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.258 | Val loss: 0.321 | Gen: etay airway onditioningway isway orkingway\n",
            "Epoch:  27 | Train loss: 0.189 | Val loss: 0.254 | Gen: eththhthhthhay airray onditioningway isway orkingway\n",
            "Epoch:  28 | Train loss: 0.151 | Val loss: 0.233 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  29 | Train loss: 0.189 | Val loss: 0.281 | Gen: etay airway onditioningway isway orkingway\n",
            "Epoch:  30 | Train loss: 0.174 | Val loss: 0.286 | Gen: eththhthhay airway onditioningway issay orkingway\n",
            "Epoch:  31 | Train loss: 0.143 | Val loss: 0.227 | Gen: eththhthhay airway onditioningway isway orkingway\n",
            "Epoch:  32 | Train loss: 0.152 | Val loss: 0.289 | Gen: etay airway onditioningway isway orkingway\n",
            "Epoch:  33 | Train loss: 0.129 | Val loss: 0.204 | Gen: eththhthhay airway onditioningway isway orkingway\n",
            "Epoch:  34 | Train loss: 0.099 | Val loss: 0.198 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  35 | Train loss: 0.094 | Val loss: 0.191 | Gen: eththhthaykay airway onditioningway isway orkingway\n",
            "Epoch:  36 | Train loss: 0.109 | Val loss: 0.701 | Gen: eththay airway onditiongnay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.293 | Val loss: 0.308 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.170 | Val loss: 0.224 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  39 | Train loss: 0.154 | Val loss: 0.264 | Gen: eththay airway onditioningway isway orkingway\n",
            "Epoch:  40 | Train loss: 0.110 | Val loss: 0.176 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  41 | Train loss: 0.082 | Val loss: 0.194 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  42 | Train loss: 0.069 | Val loss: 0.164 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.059 | Val loss: 0.315 | Gen: etay airway onditioningway isway orkingway\n",
            "Epoch:  44 | Train loss: 0.166 | Val loss: 0.281 | Gen: etay airway onditioningway isway orkingway\n",
            "Epoch:  45 | Train loss: 0.082 | Val loss: 0.138 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.078 | Val loss: 0.129 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.054 | Val loss: 0.121 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.042 | Val loss: 0.114 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.038 | Val loss: 0.107 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.036 | Val loss: 0.112 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  51 | Train loss: 0.033 | Val loss: 0.127 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  52 | Train loss: 0.035 | Val loss: 0.109 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  53 | Train loss: 0.166 | Val loss: 1.216 | Gen: ethay inray onditiongingcay isway orkinway\n",
            "Epoch:  54 | Train loss: 0.495 | Val loss: 0.359 | Gen: etay airway onditionganingay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.171 | Val loss: 0.167 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  56 | Train loss: 0.087 | Val loss: 0.133 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.057 | Val loss: 0.111 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.044 | Val loss: 0.099 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.036 | Val loss: 0.097 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.033 | Val loss: 0.092 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.028 | Val loss: 0.090 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.027 | Val loss: 0.093 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.024 | Val loss: 0.090 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.022 | Val loss: 0.092 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.020 | Val loss: 0.090 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.020 | Val loss: 0.090 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.019 | Val loss: 0.101 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.020 | Val loss: 0.103 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.171 | Val loss: 0.219 | Gen: etay airway onditioningway isway orkingway\n",
            "Epoch:  70 | Train loss: 0.132 | Val loss: 0.267 | Gen: etay airway onditiongway isway orkingway\n",
            "Epoch:  71 | Train loss: 0.082 | Val loss: 0.136 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.035 | Val loss: 0.116 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.025 | Val loss: 0.113 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.019 | Val loss: 0.108 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.017 | Val loss: 0.110 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.016 | Val loss: 0.102 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.014 | Val loss: 0.101 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.012 | Val loss: 0.101 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.011 | Val loss: 0.101 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.011 | Val loss: 0.097 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.010 | Val loss: 0.096 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.009 | Val loss: 0.096 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.009 | Val loss: 0.097 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.008 | Val loss: 0.091 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.008 | Val loss: 0.093 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.009 | Val loss: 0.139 | Gen: etay airway onditioningway isway orkingway\n",
            "Epoch:  87 | Train loss: 0.156 | Val loss: 0.176 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.098 | Val loss: 0.205 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  89 | Train loss: 0.048 | Val loss: 0.129 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.023 | Val loss: 0.106 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.019 | Val loss: 0.103 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.015 | Val loss: 0.095 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.013 | Val loss: 0.103 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.010 | Val loss: 0.095 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.008 | Val loss: 0.091 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.007 | Val loss: 0.088 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.006 | Val loss: 0.088 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.006 | Val loss: 0.088 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.005 | Val loss: 0.089 | Gen: etay airway onditioningcay isway orkingway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tetay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vE-hKCxhF3iR",
        "colab_type": "code",
        "outputId": "11e811ff-56d0-4ada-8294-38c88697e139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the british-born sixty nine year old is known as the godfather of the strain of artificial intelligence called neural networks or neural nets which involves setting up computer systems to mimic the human brain allowing them to learn it is some experts say going to radically transform our lives already is actually the way electricity did'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe british-born sixty nine year old is known as the godfather of the strain of artificial intelligence called neural networks or neural nets which involves setting up computer systems to mimic the human brain allowing them to learn it is some experts say going to radically transform our lives already is actually the way electricity did \n",
            "translated:\tethay itishbay-ornbay ixtysay inenay yearay oldway isway ownknay asway ethay odfathergay ofway ethay ainstray ofway artificialway intelligenceway alledcay euralnay etworksnay orway euralnay etsnay ichwhay involvesway ettingsay upway omputercay ystemssay otay imicmay ethay umanhay ainbray allowingway emthay otay earnlay itway isway omesay expertsway aysay oinggay otay adicallyray ansformtray ourway iveslay alreadyway isway actuallyway ethay ayway electricityway idday\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YtDrsRU3sb8S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2193
        },
        "outputId": "99a39340-1194-40da-e4e4-a9ec50c29d6d"
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'scaled_dot',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_attn_encoder, rnn_attn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: rnn_attention                          \n",
            "                               lr_decay: 0.99                                   \n",
            "                         attention_type: scaled_dot                             \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.353 | Val loss: 1.965 | Gen: engay ay iningtingtingtingtay ingay ingay\n",
            "Epoch:   1 | Train loss: 1.833 | Val loss: 1.714 | Gen: eedray ay ongungingingsay ingay oongsingay\n",
            "Epoch:   2 | Train loss: 1.620 | Val loss: 1.546 | Gen: edway aningay ongingingay ingay oongay\n",
            "Epoch:   3 | Train loss: 1.440 | Val loss: 1.388 | Gen: edsay aniray ontintingray indway olingay\n",
            "Epoch:   4 | Train loss: 1.282 | Val loss: 1.237 | Gen: esay aingay onditiningay isshay ortingay\n",
            "Epoch:   5 | Train loss: 1.132 | Val loss: 1.151 | Gen: eagshthththththththt aingray ondiningnay isshay orsingway\n",
            "Epoch:   6 | Train loss: 1.004 | Val loss: 0.971 | Gen: eaysethtay ainway onditingay isway orcingay\n",
            "Epoch:   7 | Train loss: 0.874 | Val loss: 0.955 | Gen: eway aingray ondiitingnay issay ordingnay\n",
            "Epoch:   8 | Train loss: 0.751 | Val loss: 0.776 | Gen: eway airway ondiitingray isway orkingdray\n",
            "Epoch:   9 | Train loss: 0.684 | Val loss: 0.711 | Gen: eway airray onditingnay isway orkingpay\n",
            "Epoch:  10 | Train loss: 0.576 | Val loss: 0.623 | Gen: eway airaray ondiingnay isway orkingway\n",
            "Epoch:  11 | Train loss: 0.473 | Val loss: 0.543 | Gen: eway airway onditiongnay isway orkingway\n",
            "Epoch:  12 | Train loss: 0.447 | Val loss: 0.529 | Gen: efay airway onditiningnay isway orkingway\n",
            "Epoch:  13 | Train loss: 0.394 | Val loss: 0.485 | Gen: efay airway onditiongnay isway orkingway\n",
            "Epoch:  14 | Train loss: 0.320 | Val loss: 0.419 | Gen: eway airway onditiongnay isway orkingway\n",
            "Epoch:  15 | Train loss: 0.297 | Val loss: 0.422 | Gen: efay airway onditiongnay isway orkingway\n",
            "Epoch:  16 | Train loss: 0.287 | Val loss: 0.386 | Gen: eway airway onditiongnay issay orkingway\n",
            "Epoch:  17 | Train loss: 0.249 | Val loss: 0.370 | Gen: ekay airway onditiongnay isway orkingway\n",
            "Epoch:  18 | Train loss: 0.277 | Val loss: 0.383 | Gen: ewhay airway onditiongray isway orkingway\n",
            "Epoch:  19 | Train loss: 0.283 | Val loss: 0.392 | Gen: efay aisway onditiongcay isway orkingay\n",
            "Epoch:  20 | Train loss: 0.265 | Val loss: 0.347 | Gen: ehay airway onditiongcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.208 | Val loss: 0.350 | Gen: efay airway onditiongcay issway orkingay\n",
            "Epoch:  22 | Train loss: 0.200 | Val loss: 0.319 | Gen: ehthay airway onditiongcay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.169 | Val loss: 0.286 | Gen: efay airway onditiongcay issway orkingway\n",
            "Epoch:  24 | Train loss: 0.148 | Val loss: 0.297 | Gen: ewhay airway onditiongcay isway orkingay\n",
            "Epoch:  25 | Train loss: 0.135 | Val loss: 0.266 | Gen: efay airway onditiongcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.140 | Val loss: 0.280 | Gen: efay airway onditiongcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.134 | Val loss: 0.253 | Gen: efay airway onditiongcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.141 | Val loss: 0.465 | Gen: ekay airway onditiongcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.257 | Val loss: 0.288 | Gen: eway airway onditiongcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.168 | Val loss: 0.316 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.119 | Val loss: 0.242 | Gen: efay airway onditiongcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.121 | Val loss: 0.255 | Gen: efay airway onditiongcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.143 | Val loss: 0.234 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.132 | Val loss: 0.288 | Gen: ewhay airway onditiongcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.135 | Val loss: 0.275 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.101 | Val loss: 0.223 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.110 | Val loss: 0.293 | Gen: ewhay airway onditiongcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.168 | Val loss: 0.260 | Gen: ewhay airway onditiongcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.092 | Val loss: 0.233 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.081 | Val loss: 0.223 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.066 | Val loss: 0.229 | Gen: eghay airway onditiongcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.060 | Val loss: 0.207 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.059 | Val loss: 0.214 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.055 | Val loss: 0.203 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.055 | Val loss: 0.217 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.073 | Val loss: 0.224 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.074 | Val loss: 0.220 | Gen: ekay airway onditiongcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.069 | Val loss: 0.209 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.062 | Val loss: 0.212 | Gen: ekay airway onditiongncay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.080 | Val loss: 0.199 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  51 | Train loss: 0.112 | Val loss: 0.427 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  52 | Train loss: 0.266 | Val loss: 0.322 | Gen: edhay airway onditiongcay isway orkingway\n",
            "Epoch:  53 | Train loss: 0.217 | Val loss: 0.340 | Gen: edhay airway onditiongcay isway orkingway\n",
            "Epoch:  54 | Train loss: 0.122 | Val loss: 0.297 | Gen: ekay airway onditiongcay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.092 | Val loss: 0.225 | Gen: edhay airway onditiongcay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.065 | Val loss: 0.221 | Gen: edhay airway onditiongcay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.047 | Val loss: 0.194 | Gen: edhay airway onditioningcay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.037 | Val loss: 0.189 | Gen: edhay airway onditioningcay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.032 | Val loss: 0.191 | Gen: edhay airway onditioningcay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.029 | Val loss: 0.204 | Gen: edhay airway onditioningcay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.026 | Val loss: 0.202 | Gen: edhay airway onditioningcay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.025 | Val loss: 0.200 | Gen: eghay airway onditioningcay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.023 | Val loss: 0.203 | Gen: eghay airway onditioningcay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.022 | Val loss: 0.195 | Gen: eghay airway onditioningcay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.021 | Val loss: 0.200 | Gen: efay airway onditioningcay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.034 | Val loss: 0.206 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.060 | Val loss: 0.267 | Gen: ekay airway onditiongncay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.102 | Val loss: 0.262 | Gen: egshay airway onditioningcay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.189 | Val loss: 0.470 | Gen: esthay airray onditiongnay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.202 | Val loss: 0.256 | Gen: esthay airray onditiongcay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.106 | Val loss: 0.186 | Gen: ehay airway onditiongcay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.041 | Val loss: 0.161 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.030 | Val loss: 0.164 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.025 | Val loss: 0.167 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.022 | Val loss: 0.169 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.020 | Val loss: 0.168 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.019 | Val loss: 0.166 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.017 | Val loss: 0.165 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.016 | Val loss: 0.166 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.017 | Val loss: 0.158 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.019 | Val loss: 0.183 | Gen: ehay airway onditiongcay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.022 | Val loss: 0.173 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.024 | Val loss: 0.184 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.021 | Val loss: 0.183 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.029 | Val loss: 0.195 | Gen: ehay airway onditiongcay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.025 | Val loss: 0.199 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.024 | Val loss: 0.194 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.086 | Val loss: 0.345 | Gen: efay airway onditiongcay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.134 | Val loss: 0.248 | Gen: ehay airway onditionichay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.067 | Val loss: 0.255 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.047 | Val loss: 0.222 | Gen: ehay airway onditiongcay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.023 | Val loss: 0.187 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.016 | Val loss: 0.201 | Gen: ehay airway onditioningcay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.014 | Val loss: 0.201 | Gen: efhay airway onditioningcay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.012 | Val loss: 0.210 | Gen: efay airway onditioningcay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.011 | Val loss: 0.210 | Gen: efay airway onditioningcay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.010 | Val loss: 0.214 | Gen: efay airway onditioningcay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.010 | Val loss: 0.214 | Gen: efay airway onditioningcay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.009 | Val loss: 0.215 | Gen: efay airway onditioningcay isway orkingway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tefay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1vYOZ7_1skMG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X8FaZZUWRpY9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ]
    },
    {
      "metadata": {
        "id": "Ik5rx9qw9KCg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2295
        },
        "outputId": "0c0fa084-4bbb-4f15-84a2-29d179a383c6"
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "transformer_encoder, transformer_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                 num_transformer_layers: 3                                      \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: transformer                            \n",
            "                               lr_decay: 0.99                                   \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type TransformerDecoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type CausalScaledDotAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type ScaledDotAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0 | Train loss: 2.199 | Val loss: 1.730 | Gen: eray iray oningngngngngngngngn isisisisisisisisisis ongay\n",
            "Epoch:   1 | Train loss: 1.601 | Val loss: 1.359 | Gen: eththay airay oningingngingngnging isissisgsgsgsgsgsisg otinggray\n",
            "Epoch:   2 | Train loss: 1.229 | Val loss: 1.087 | Gen: ehthay airay onginglay isgrisgsgray orcingray\n",
            "Epoch:   3 | Train loss: 0.963 | Val loss: 0.878 | Gen: ehay airay ongingway isway orkingway\n",
            "Epoch:   4 | Train loss: 0.791 | Val loss: 0.717 | Gen: ethay airway ondiongiongway isway orkingway\n",
            "Epoch:   5 | Train loss: 0.598 | Val loss: 0.618 | Gen: ethay airway onininingway isway orkingway\n",
            "Epoch:   6 | Train loss: 0.526 | Val loss: 0.538 | Gen: ethay airway onininingway isway orkingway\n",
            "Epoch:   7 | Train loss: 0.403 | Val loss: 0.496 | Gen: ehay airay onditingway isway orkingway\n",
            "Epoch:   8 | Train loss: 0.374 | Val loss: 0.438 | Gen: ethay airay ondingcay isway orkingway\n",
            "Epoch:   9 | Train loss: 0.337 | Val loss: 0.392 | Gen: ethay airay onditingcay isway orkingway\n",
            "Epoch:  10 | Train loss: 0.327 | Val loss: 0.434 | Gen: ethay airay onditininininininini isway orkingway\n",
            "Epoch:  11 | Train loss: 0.278 | Val loss: 0.309 | Gen: ethay airay onditininininininini isway orkingway\n",
            "Epoch:  12 | Train loss: 0.263 | Val loss: 0.272 | Gen: ethay airway onditininininininini isway orkingway\n",
            "Epoch:  13 | Train loss: 0.304 | Val loss: 0.388 | Gen: ethay airway onditiondway isway orkingway\n",
            "Epoch:  14 | Train loss: 0.321 | Val loss: 0.453 | Gen: ehay airway onditingcay isway orpingway\n",
            "Epoch:  15 | Train loss: 0.264 | Val loss: 0.275 | Gen: ethay airway onditiningcay isway orkingway\n",
            "Epoch:  16 | Train loss: 0.201 | Val loss: 0.290 | Gen: ethay airway ondinininingcay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.212 | Val loss: 0.215 | Gen: ethay airway onditioniniongcay isway orkingway\n",
            "Epoch:  18 | Train loss: 0.121 | Val loss: 0.168 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.096 | Val loss: 0.213 | Gen: ethay airway onditioniongcay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.157 | Val loss: 0.253 | Gen: ethay airway onditiningcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.180 | Val loss: 0.281 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.170 | Val loss: 0.191 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.100 | Val loss: 0.146 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.095 | Val loss: 0.219 | Gen: ethay airway ondingcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.124 | Val loss: 0.185 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.165 | Val loss: 0.247 | Gen: ethay airway ondiongcay isway okingway\n",
            "Epoch:  27 | Train loss: 0.118 | Val loss: 0.136 | Gen: ethay airway onditiningcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.076 | Val loss: 0.149 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.074 | Val loss: 0.129 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.054 | Val loss: 0.114 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.038 | Val loss: 0.101 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.122 | Val loss: 0.126 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.068 | Val loss: 0.118 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.052 | Val loss: 0.171 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.063 | Val loss: 0.136 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.080 | Val loss: 0.151 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.058 | Val loss: 0.134 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.125 | Val loss: 0.174 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.068 | Val loss: 0.152 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.098 | Val loss: 0.228 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.127 | Val loss: 0.148 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.087 | Val loss: 0.107 | Gen: ethay airway onditiongway isway orkingway\n",
            "Epoch:  43 | Train loss: 0.060 | Val loss: 0.101 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.030 | Val loss: 0.072 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.031 | Val loss: 0.090 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.017 | Val loss: 0.064 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.041 | Val loss: 0.230 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.070 | Val loss: 0.146 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.054 | Val loss: 0.094 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.027 | Val loss: 0.178 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  51 | Train loss: 0.197 | Val loss: 0.161 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  52 | Train loss: 0.072 | Val loss: 0.110 | Gen: ethay airway onditiongway isway orkingway\n",
            "Epoch:  53 | Train loss: 0.049 | Val loss: 0.089 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  54 | Train loss: 0.040 | Val loss: 0.105 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.076 | Val loss: 0.113 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.038 | Val loss: 0.102 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.019 | Val loss: 0.081 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.018 | Val loss: 0.097 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.024 | Val loss: 0.093 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.026 | Val loss: 0.104 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.014 | Val loss: 0.103 | Gen: ethay airway onditiongway isway orkingway\n",
            "Epoch:  62 | Train loss: 0.013 | Val loss: 0.086 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.008 | Val loss: 0.091 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.011 | Val loss: 0.085 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.006 | Val loss: 0.082 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.065 | Val loss: 0.111 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.043 | Val loss: 0.098 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.053 | Val loss: 0.140 | Gen: ethay airway onditioninininininin isway orkingway\n",
            "Epoch:  69 | Train loss: 0.070 | Val loss: 0.131 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.024 | Val loss: 0.082 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.012 | Val loss: 0.058 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.010 | Val loss: 0.066 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.012 | Val loss: 0.089 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.028 | Val loss: 0.074 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.045 | Val loss: 0.118 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.035 | Val loss: 0.070 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.022 | Val loss: 0.069 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.108 | Val loss: 0.264 | Gen: ethay airway onditioninway isway okingway\n",
            "Epoch:  79 | Train loss: 0.098 | Val loss: 0.161 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.043 | Val loss: 0.100 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.018 | Val loss: 0.100 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.014 | Val loss: 0.098 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.008 | Val loss: 0.090 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.008 | Val loss: 0.110 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.016 | Val loss: 0.105 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.005 | Val loss: 0.100 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.004 | Val loss: 0.104 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.003 | Val loss: 0.104 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.003 | Val loss: 0.107 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.002 | Val loss: 0.105 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.003 | Val loss: 0.114 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.006 | Val loss: 0.108 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.009 | Val loss: 0.125 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.052 | Val loss: 0.196 | Gen: ethay airway onditiningcay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.082 | Val loss: 0.133 | Gen: ethay airway onditiningcay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.048 | Val loss: 0.096 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.038 | Val loss: 0.097 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.019 | Val loss: 0.078 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.010 | Val loss: 0.081 | Gen: ethay airway onditiongcay isway orkingway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditiongcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ULCMHm5ZF7vx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "33bdae9e-9877-4b35-d234-89be43c2ce12"
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditiongcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qfUMVN0nwpQO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2312
        },
        "outputId": "046089ca-e653-4409-d687-bf732da47bdc"
      },
      "cell_type": "code",
      "source": [
        "print(\"============ These results will be those using non-causal attention ============\")\n",
        "\n",
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "transformer_encoder, transformer_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============ These results will be those using non-causal attention ============\n",
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                 num_transformer_layers: 3                                      \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: transformer                            \n",
            "                               lr_decay: 0.99                                   \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type GRUEncoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type TransformerDecoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type ScaledDotAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0 | Train loss: 2.174 | Val loss: 1.714 | Gen: tttttttttttttttttttt rrrrrrrrrrrrrrrrrrrr  yyyyyyyyyyyyyyyyyyyy ay\n",
            "Epoch:   1 | Train loss: 1.605 | Val loss: 1.394 | Gen:  alalalalalalalalalal  - \n",
            "Epoch:   2 | Train loss: 1.302 | Val loss: 1.160 | Gen: eththththththththtth ariririririririririr  -------------------- \n",
            "Epoch:   3 | Train loss: 1.091 | Val loss: 0.948 | Gen:  my onayEOSayay isisisisisisisisisis orgrgrgrgrgrgrggrgrg\n",
            "Epoch:   4 | Train loss: 0.916 | Val loss: 0.840 | Gen: ethay aaaaaaaaaaaaaaaaaaaa onininininininininin isisy orkinkinkinkinkinkik\n",
            "Epoch:   5 | Train loss: 0.877 | Val loss: 0.774 | Gen: ethethethethethethet airpairpairpairpairp onininininininininin isisisisisisisisisis oraEOSy\n",
            "Epoch:   6 | Train loss: 0.690 | Val loss: 0.662 | Gen: ethethethethethethet  onininininininininin isisisisisisisisisis oraaayEOSy\n",
            "Epoch:   7 | Train loss: 0.630 | Val loss: 0.673 | Gen: ethaaEOSayayyy airairairairairairai onininininininininin isisisisisisisisisis orgingingingingingin\n",
            "Epoch:   8 | Train loss: 0.580 | Val loss: 0.597 | Gen: eay  oniy isisisisisisisisisis orgcay\n",
            "Epoch:   9 | Train loss: 0.534 | Val loss: 0.589 | Gen: eay airairairairairairar oniy isisisisisisisisisis orgcay\n",
            "Epoch:  10 | Train loss: 0.512 | Val loss: 0.553 | Gen: ethethethethethethet   isisisisisisisisisis orgcay\n",
            "Epoch:  11 | Train loss: 0.469 | Val loss: 0.514 | Gen: ethayEOSty ayEOSy oniy isay orgcay\n",
            "Epoch:  12 | Train loss: 0.455 | Val loss: 0.514 | Gen: eay ay oniy isay orgcay\n",
            "Epoch:  13 | Train loss: 0.448 | Val loss: 0.527 | Gen: ethaty airairairairairairai ocay isay orkingway\n",
            "Epoch:  14 | Train loss: 0.432 | Val loss: 0.485 | Gen: eay   isay orgcay   \n",
            "Epoch:  15 | Train loss: 0.411 | Val loss: 0.471 | Gen: eay ay oniy isay orkingwayEOSyEOSEOSEOSyy\n",
            "Epoch:  16 | Train loss: 0.405 | Val loss: 0.467 | Gen: ethay ay ocay isay orgay\n",
            "Epoch:  17 | Train loss: 0.419 | Val loss: 0.534 | Gen: ethaty airairairairairairar ocay isay orkway\n",
            "Epoch:  18 | Train loss: 0.437 | Val loss: 0.471 | Gen: ethy    orgcorgcorgcorgcorgc\n",
            "Epoch:  19 | Train loss: 0.418 | Val loss: 0.486 | Gen: eay ay oniy isay orkingway\n",
            "Epoch:  20 | Train loss: 0.399 | Val loss: 0.449 | Gen: ety ay  isay orgay  \n",
            "Epoch:  21 | Train loss: 0.379 | Val loss: 0.456 | Gen: ethay ay ocay isay orkingwny\n",
            "Epoch:  22 | Train loss: 0.403 | Val loss: 0.458 | Gen: ethy ay ocay isay orkingwny\n",
            "Epoch:  23 | Train loss: 0.406 | Val loss: 0.455 | Gen:  ay  isay orkingorkingorkingor\n",
            "Epoch:  24 | Train loss: 0.410 | Val loss: 0.436 | Gen: ethy ay y isay orkingway\n",
            "Epoch:  25 | Train loss: 0.388 | Val loss: 0.442 | Gen: ethy ay ongy isay orkingway\n",
            "Epoch:  26 | Train loss: 0.366 | Val loss: 0.423 | Gen: etay airairairairairairai y isay y\n",
            "Epoch:  27 | Train loss: 0.366 | Val loss: 0.434 | Gen: ethy ay ocay isay orkingwayEOSEOSy\n",
            "Epoch:  28 | Train loss: 0.363 | Val loss: 0.437 | Gen: ethathathathathathah airairairairairairai oniy isay orkay\n",
            "Epoch:  29 | Train loss: 0.375 | Val loss: 0.445 | Gen: ethy ay oniy isay orkingwayEOSEOSy\n",
            "Epoch:  30 | Train loss: 0.374 | Val loss: 0.424 | Gen: etay arararararararararar oniy isay orkingway\n",
            "Epoch:  31 | Train loss: 0.361 | Val loss: 0.435 | Gen: ethy ay oniy isay orkingway\n",
            "Epoch:  32 | Train loss: 0.367 | Val loss: 0.406 | Gen: etay arararararararararar  isay orkingwayEOSEOSy\n",
            "Epoch:  33 | Train loss: 0.365 | Val loss: 0.432 | Gen: ethy arararararararararar ocay isay orkingway\n",
            "Epoch:  34 | Train loss: 0.367 | Val loss: 0.427 | Gen: ethy ay ony isay orkiy\n",
            "Epoch:  35 | Train loss: 0.349 | Val loss: 0.413 | Gen: ethy ay y isay y    \n",
            "Epoch:  36 | Train loss: 0.343 | Val loss: 0.401 | Gen:    isay orkingay    \n",
            "Epoch:  37 | Train loss: 0.342 | Val loss: 0.419 | Gen: ethaEOSty airairairairairairai onionionionionnionni isay orkingway\n",
            "Epoch:  38 | Train loss: 0.343 | Val loss: 0.407 | Gen: ethy ay oniy isay orkingway\n",
            "Epoch:  39 | Train loss: 0.340 | Val loss: 0.396 | Gen: ethaEOSty airairairairairairar ocaycay isay orkingway\n",
            "Epoch:  40 | Train loss: 0.334 | Val loss: 0.401 | Gen: ehay  oniy isay orkingway\n",
            "Epoch:  41 | Train loss: 0.340 | Val loss: 0.416 | Gen: ethaEOSty airairairairairairar onay isay orkingway\n",
            "Epoch:  42 | Train loss: 0.341 | Val loss: 0.403 | Gen: ehay  y isay orkingway\n",
            "Epoch:  43 | Train loss: 0.342 | Val loss: 0.396 | Gen: ehay arararararararararar oniy isay orkingway\n",
            "Epoch:  44 | Train loss: 0.333 | Val loss: 0.395 | Gen: ehay  y isay orkingway\n",
            "Epoch:  45 | Train loss: 0.338 | Val loss: 0.396 | Gen: ehay airairairairairairai onionionionionionion isay orkingway\n",
            "Epoch:  46 | Train loss: 0.331 | Val loss: 0.407 | Gen: ethaEOSty ay oniy  orkingway\n",
            "Epoch:  47 | Train loss: 0.333 | Val loss: 0.409 | Gen: ehay airairairairairairai oniy isay orkingway\n",
            "Epoch:  48 | Train loss: 0.336 | Val loss: 0.394 | Gen: ehay airairairairairairai ondy isay orkingway\n",
            "Epoch:  49 | Train loss: 0.335 | Val loss: 0.394 | Gen: ethaEOSty airairairairairairai onay isay orkingway\n",
            "Epoch:  50 | Train loss: 0.330 | Val loss: 0.388 | Gen: ehay airairairairairairai onionionionionionion isay orkingway\n",
            "Epoch:  51 | Train loss: 0.328 | Val loss: 0.388 | Gen: ethaEOSty airairairairairairai oniy isay orkingway\n",
            "Epoch:  52 | Train loss: 0.328 | Val loss: 0.384 | Gen: ehay airairairairairairai onionionionionionion isay orkingway\n",
            "Epoch:  53 | Train loss: 0.331 | Val loss: 0.389 | Gen: ethaEOSty  oniy isay orkingway\n",
            "Epoch:  54 | Train loss: 0.333 | Val loss: 0.420 | Gen: ehay airairairairairairai onionionionionionion isay orkingway\n",
            "Epoch:  55 | Train loss: 0.333 | Val loss: 0.398 | Gen: ehay airairairairairairai oniy isay orkingway\n",
            "Epoch:  56 | Train loss: 0.332 | Val loss: 0.396 | Gen: ehay airairairairairairai onionionionionioonio isay orkingay\n",
            "Epoch:  57 | Train loss: 0.326 | Val loss: 0.391 | Gen:   onionionionionionion  \n",
            "Epoch:  58 | Train loss: 0.319 | Val loss: 0.385 | Gen:                     \n",
            "Epoch:  59 | Train loss: 0.314 | Val loss: 0.372 | Gen: ehay airairairairairairai oniy isay orkingway\n",
            "Epoch:  60 | Train loss: 0.310 | Val loss: 0.380 | Gen:    isay orkingway   \n",
            "Epoch:  61 | Train loss: 0.316 | Val loss: 0.378 | Gen: ehay   isay orkingway\n",
            "Epoch:  62 | Train loss: 0.314 | Val loss: 0.387 | Gen: ethaEOSty  onaynay isay orkingway\n",
            "Epoch:  63 | Train loss: 0.316 | Val loss: 0.387 | Gen: ehay airairairairairairai oniy isay orkingway\n",
            "Epoch:  64 | Train loss: 0.309 | Val loss: 0.378 | Gen: ethaEOSty  oniy  orkingway\n",
            "Epoch:  65 | Train loss: 0.311 | Val loss: 0.371 | Gen:   onay isay orkingway\n",
            "Epoch:  66 | Train loss: 0.318 | Val loss: 0.385 | Gen: ethy                \n",
            "Epoch:  67 | Train loss: 0.319 | Val loss: 0.380 | Gen: ethaEOSty   isay orkingway\n",
            "Epoch:  68 | Train loss: 0.312 | Val loss: 0.377 | Gen: ethy                \n",
            "Epoch:  69 | Train loss: 0.315 | Val loss: 0.377 | Gen: ehay  ocay isay orkingway\n",
            "Epoch:  70 | Train loss: 0.311 | Val loss: 0.384 | Gen: ethy  oniy isay orkingway\n",
            "Epoch:  71 | Train loss: 0.312 | Val loss: 0.377 | Gen: ehay   isay         \n",
            "Epoch:  72 | Train loss: 0.316 | Val loss: 0.384 | Gen: ethy   isay orkiny  \n",
            "Epoch:  73 | Train loss: 0.313 | Val loss: 0.375 | Gen: ehay    orkingway   \n",
            "Epoch:  74 | Train loss: 0.309 | Val loss: 0.369 | Gen: ehay  onionionionionionion isay orkingway\n",
            "Epoch:  75 | Train loss: 0.306 | Val loss: 0.368 | Gen: ehay                \n",
            "Epoch:  76 | Train loss: 0.303 | Val loss: 0.369 | Gen:  airairairairairairai  isay orkingway\n",
            "Epoch:  77 | Train loss: 0.303 | Val loss: 0.370 | Gen: ehay   isay orkingway\n",
            "Epoch:  78 | Train loss: 0.308 | Val loss: 0.383 | Gen:    isay orkingway   \n",
            "Epoch:  79 | Train loss: 0.310 | Val loss: 0.373 | Gen: ethy   isay orkiny  \n",
            "Epoch:  80 | Train loss: 0.306 | Val loss: 0.362 | Gen: ehay airairairairairairai  isay orkingway\n",
            "Epoch:  81 | Train loss: 0.302 | Val loss: 0.360 | Gen: ethy   isay orkingway\n",
            "Epoch:  82 | Train loss: 0.299 | Val loss: 0.367 | Gen:  airairairairairairai  isay orkingway\n",
            "Epoch:  83 | Train loss: 0.301 | Val loss: 0.375 | Gen: ethy  onionionionionionion isay orkingway\n",
            "Epoch:  84 | Train loss: 0.308 | Val loss: 0.365 | Gen:    isay orkiny      \n",
            "Epoch:  85 | Train loss: 0.305 | Val loss: 0.364 | Gen: ethy   isay orkiny  \n",
            "Epoch:  86 | Train loss: 0.301 | Val loss: 0.366 | Gen:    isay             \n",
            "Epoch:  87 | Train loss: 0.304 | Val loss: 0.361 | Gen: ethy  onionionionionionion isay orkingway\n",
            "Epoch:  88 | Train loss: 0.302 | Val loss: 0.368 | Gen:   onionionionionionion isay orkiny\n",
            "Epoch:  89 | Train loss: 0.301 | Val loss: 0.370 | Gen:    isay orkingway   \n",
            "Epoch:  90 | Train loss: 0.300 | Val loss: 0.360 | Gen:    isay             \n",
            "Epoch:  91 | Train loss: 0.296 | Val loss: 0.357 | Gen:    isay orkingway   \n",
            "Epoch:  92 | Train loss: 0.296 | Val loss: 0.358 | Gen:    isay orkiny      \n",
            "Epoch:  93 | Train loss: 0.294 | Val loss: 0.355 | Gen:    isay             \n",
            "Epoch:  94 | Train loss: 0.298 | Val loss: 0.359 | Gen:    isay orkingway   \n",
            "Epoch:  95 | Train loss: 0.298 | Val loss: 0.362 | Gen:    isay orkiny      \n",
            "Epoch:  96 | Train loss: 0.299 | Val loss: 0.356 | Gen:    isay             \n",
            "Epoch:  97 | Train loss: 0.299 | Val loss: 0.358 | Gen:    isay             \n",
            "Epoch:  98 | Train loss: 0.301 | Val loss: 0.358 | Gen: ehay   isay         \n",
            "Epoch:  99 | Train loss: 0.300 | Val loss: 0.363 | Gen:    isay             \n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\t   isay \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hT96_mzWwpBN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "67e4c548-24e8-4950-e6ca-b8ed43f6a4c0"
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\t   isay \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qbfZCByITOI6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Attention visualization"
      ]
    },
    {
      "metadata": {
        "id": "itCGMv3FdXsn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TEST_WORD_ATTN = 'pneumonoultramicroscopicsilicovolcanoconiosis'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xBv4QQuBiU-V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize RNN attention map"
      ]
    },
    {
      "metadata": {
        "id": "aXvqoQYONMTA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "813a38ef-5e34-40ff-c288-0763ba8a4251"
      },
      "cell_type": "code",
      "source": [
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARQAAAGACAYAAABsh50sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eumonoultramociconca'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "xuOvxfA1NMz3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize transformer attention maps from all the transformer layers"
      ]
    },
    {
      "metadata": {
        "id": "HSSB4wd8-M7g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1186
        },
        "outputId": "3f2c390e-9109-47b4-eee5-0f71ed5d067b"
      },
      "cell_type": "code",
      "source": [
        "visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args, )"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARQAAAGACAYAAABsh50sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARQAAAGACAYAAABsh50sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbiihEELs5v8BSeuVyNSDAp8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARQAAAGACAYAAABsh50sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noYRCCLGa/w/nFfj2nuo4NAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eumonomopicopanicani'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "nj6R5xzqKjdg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}